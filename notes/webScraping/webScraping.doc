Make sure you are only making one request per page.

open() - storing content locally. Argument wb stands for "write bytes".

Create function.

--code start--
def save_html(html, path):
    with open(path, 'wb') as f:
        f.write(html)

save_html(r.content, 'google_com')
--code end--

'rb' stands for read bytes.


--code start--
def open_html(path):

    with open(path, 'rb') as f:
    
        return f.read()

html = open_html('google_com')
--code end--

example.com/robots.txt has the guidelines for bots and scrapers.

User-agent: * applies to all.
Crawl-delay: number of seconds to wait between requests.
Allow: the pages we are allowed to request from
Disallow are the pages we are not allowed to scrape.

pip install requests


--SC--
import requests

url = '<url>'

r = requests.get(url)

print(r.content[:100])
--CE--

pip install beautifulsoup4


--SC--
from bs4 import BeautifulSoup

soup = BeautifulSoup(r.content, 'html.parser')
--CE--

--SC--
rows = soup.select('parser')
--CE--

--SC--
row = rows[0]

name = row.select_one('.source-title').text.strip()

print(name)
--CE--

--SC--
data = []

for row in rows:
        d = dict()

        d['name'] = row.select_one(<class>).text.strip()
        .
        .
        .
        .

        data.append(d)